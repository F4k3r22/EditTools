Text_input_test = """
Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model Chunting Zhouµ∗ Michihiro Yasunagaµ Lili Yuµ∗ Arun Babuδ† Leonid Shamisµ Kushal Tirumalaµ Jacob Kahnµ Luke Zettlemoyerµ arXiv:2408.11039v1  [cs.AI]  20 Aug 2024 Omer Levy† µ Meta δ Waymoσ University of Southern California Abstract Xuezhe Maσ Weintroduce Transfusion, a recipe for training a multi-modal model over discrete and continuous data. Transfusion combines the language modeling loss function (next token prediction) with diffusion to train a single transformer over mixedmodality sequences. We pretrain multiple Transfusion models up to 7B parameters from scratch on a mixture of text and image data, establishing scaling laws with respect to a variety of uni- and cross-modal benchmarks. Our experiments show that Transfusion scales significantly better than quantizing images and training a language model over discrete image tokens. By introducing modality-specific encoding and decoding layers, we can further improve the performance of Transfusion models, and even compress each image to just 16 patches. We further demonstrate that scaling our Transfusion recipe to 7B parameters and 2T multi-modal tokens produces a model that can generate images and text on a par with similar scale diffusion models and language models, reaping the benefits of both worlds. 1 Introduction Multi-modal generative models need to be able to perceive, process, and produce both discrete elements (such as text or code) and continuous elements (e.g. image, audio, and video data). While language models trained on the next token prediction objective dominate discrete modalities [OpenAI et al., 2024, Dubey et al., 2024], diffusion models [Ho et al., 2020, Rombach et al., 2022a] and their generalizations [Lipman et al., 2022] are the state of the art for generating continuous modalities [Dai et al., 2023, Esser et al., 2024b, Bar-Tal et al., 2024]. Many efforts have been made to combine these approaches, including extending a language model to use a diffusion model as a tool, either explicitly [Liu et al., 2023] or by grafting a pretrained diffusion model onto the language model [Dong et al., 2023, Koh et al., 2024]. Alternatively, one can quantize the continuous modalities [Van Den Oord et al., 2017] and train a standard language model over discrete tokens [Ramesh et al., 2021, Yu et al., 2022, 2023], simplifying the model’s architecture at the cost of losing information. In this work, we show it is possible to fully integrate both modalities, with no information loss, by training a single model to both predict discrete text tokens and diffuse continuous images. Weintroduce Transfusion, a recipe for training a model that can seamlessly generate discrete and continuous modalities. We demonstrate Transfusion by pretraining a transformer model on 50% text and 50% image data using a different objective for each modality: next token prediction for text and diffusion for images. The model is exposed to both modalities and loss functions at each training step. Standard embedding layers convert text tokens to vectors, while patchification layers represent ∗Equal contribution. †Work done while at Meta.cute cat . <BOI> What color A cute cat . <BOI> Transformer is <EOI> What color its nose ? is its nose Figure 1: A high-level illustration of Transfusion. A single transformer perceives, processes, and produces data of every modality. Discrete (text) tokens are processed autoregressively and trained on the next token prediction objective. Continuous (image) vectors are processed together in parallel and trained on the diffusion objective. Marker BOI and EOI tokens separate the modalities. each image as a sequence of patch vectors. We apply causal attention for text tokens and bidirectional attention for image patches. For inference, we introduce a decoding algorithm that combines the standard practices of text generation from language models and image generation from diffusion models. Figure 1 illustrates Transfusion. In a controlled comparison with Chameleon’s discretization approach [Chameleon Team, 2024], we show that Transfusion models scale better in every combination of modalities. In text-to-image generation, we find that Transfusion exceeds the Chameleon approach at less than a third of the compute, as measured by both FID and CLIP scores. When controlling for FLOPs, Transfusion achieves approximately 2× lower FID scores than Chameleon models. We observe a similar trend in image-to-text generation, where Transfusion matches Chameleon at 21.8% of the FLOPs. Surprisingly, Transfusion is also more efficient at learning text-to-text prediction, achieving perplexity parity on text tasks around 50% to 60% of Chameleon’s FLOPs. Ablation experiments reveal critical components and potential improvements for Transfusion. We observe that the intra-image bidirectional attention is important, and that replacing it with causal attention hurts text-to-image generation. We also find that adding U-Net down and up blocks to encode and decode images enables Transfusion to compress larger image patches with relatively small loss to performance, potentially decreasing the serving costs by up to 64×. Finally, we demonstrate that Transfusion can generate images at similar quality to other diffusion models. We train from scratch a 7B transformer enhanced with U-Net down/up layers (0.27B parameters) over 2T tokens: 1T text tokens, and approximately 5 epochs of 692M images and their captions, amounting to another 1T patches/tokens. Figure 2 shows some generated images sampled from the model. On the GenEval [Ghosh et al., 2023] benchmark, our model outperforms other popular models such as DALL-E 2 and SDXL; unlike those image generation models, it can generate text, reaching the same level of performance as Llama 1 on text benchmarks. Our experiments thus show that Transfusion is a promising approach for training truly multi-modal models. 2 Background Transfusion is a single model trained with two objectives: language modeling and diffusion. Each of these objectives represents the state of the art in discrete and continuous data modeling, respectively. This section briefly defines these objectives, as well as background on latent image representations. 2.1 Language Modeling Given a sequence of discrete tokens y = y1,...,yn from a closed vocabulary V , a language model predicts the probability of the sequence P(y). Standard language models decompose P(y) into a product of conditional probabilities n i=1 Pθ(yi|y<i). This creates an autoregressive classification task, where the probability distribution of each token yi is predicted conditioned on the prefix of a sequence y<i using a single distribution Pθ parameterized by θ. The model can be optimized by minimizing the cross-entropy between Pθ and the empirical distribution of the data, yielding the standard next-token prediction objective, colloquially referred to as LM loss: LLM =Eyi −logPθ(yi|y<i) (1) 2An armchair in the shape of an avocado A blue jay standing on a large basket of rainbow macarons. the word ‘START’ on a blue t-shirt Atransparent sculpture of a duck made out of glass. A bread, an apple, and a knife on a table “Transfusion" is written on the blackboard. A Dutch still life of an arrangement of tulips in a fluted vase. The lighting is subtle, casting gentle highlights on the flowers and emphasizing their delicate details and natural beauty. Achromeplated cat sculpture placed on a Persian rug. Acorgi. Aclose up photo of a human hand, hand model. High quality A wall in a royal castle. There are two paintings on the wall. The one on the left a detailed oil painting of the royal raccoon king. The one on the right a detailed oil painting of the royal raccoon queen. A kangaroo holding a beer, wearing ski goggles and passionately singing silly songs. human life depicted entirely out of fractals A cloud in the shape of two bunnies playing with a ball. The ball is made of clouds too. Three spheres made of glass falling into ocean. Water is splashing. Sun is setting. an egg and a bird made of wheat bread Figure 2: Generated images from a 7B Transfusion trained on 2T multi-modal tokens. 3Once trained, language models can also be used to generate text by sampling token by token from the model distribution Pθ, typically using temperature and top-p truncation. 2.2 Diffusion Denoising diffusion probabilistic models (a.k.a. DDPM or diffusion models) operate on the principle of learning to reverse a gradual noise-addition process [Ho et al., 2020]. Unlike language models that typically work with discrete tokens (y), diffusion models operate over continuous vectors (x), making them particularly suited for tasks involving continuous data like images. The diffusion framework involves two processes: a forward process that describes how the original data is turned into noise, and a reverse process of denoising that the model learns to perform. ForwardProcess Fromamathematicalperspective, the forward process defines how the noised data (which serves as the model input) is created. Given a data point x0, Ho et al. [2020] define a Markov chain that gradually adds Gaussian noise over T steps, creating a sequence of increasingly noisy versions x1,x2,..., xT. Each step of this process is defined by q(xt|xt−1) = N(xt;√1 − βtxt−1,βtI), where βt increases over time according to a predefined noise schedule (see below). This process can be reparameterized in a way that allows us to directly sample xt from x0 using a single sample of Gaussian noise ϵ ∼ N(0,I): xt = √¯ αtx0 +√ 1 − ¯ αtϵ (2) Here, ¯ αt = t s=1(1 − βs), providing a useful abstraction over the original Markov chain. In fact, both the training objective and the noise scheduler are eventually expressed (and implemented) in these terms. Reverse Process The diffusion model is trained to perform the reverse process pθ(xt−1|xt), learning to denoise the data step by step. There are several ways to do so; in this work, we follow the approach of Ho et al. [2020] and model the Gaussian noise ϵ in Equation 2 as a proxy for the cumulative noise at step t. Specifically, a model ϵθ(·) with parameters θ is trained to estimate the noise ϵ given the noised data xt and timestep t. In practice, the model often conditions on additional contextual information c, such as a caption when generating an image. The parameters of the noise prediction model are thus optimized by minimizing the mean squared error loss: LDDPM = Ex0,t,ϵ ||ϵ − ϵθ(xt,t,c)||2 (3) Noise Schedule When creating a noised example xt (Equation 2), ¯ αt determines the variance of the noise for timestep t. In this work, we adopt the commonly used cosine scheduler Nichol and Dhariwal [2021], which largely follows √¯ αt ≈ cos( t T · π 2) with some adjustments. Inference Decoding is done iteratively, pealing away some of the noise at each step. Starting with pure Gaussian noise at xT, the model ϵθ(xt,t,c) predicts the noise accumulated at timestep t. The predicted noise is then scaled according to the noise schedule, and the proportional amount of predicted noise is removed from xt to produce xt−1. In practice, inference is done over fewer timesteps than training. Classifier-free guidance (CFG) [Ho and Salimans, 2022] is often used to improve generation by contrasting the prediction of the model conditioned on the context c with the unconditioned prediction, at the cost of doubling the computation.

"""